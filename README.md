# Assistive SeeingEyes üëÅÔ∏è‚Äçüó®Ô∏èüîä
Project Link: [https://github.com/asad8210/Assistive-Seeing-Eyes](https://assistive-seeing-eyes.vercel.app/)

**Assistive SeeingEyes** is an innovative application designed to empower visually impaired individuals by providing real-time auditory feedback about their surroundings and a voice-activated personal assistant. Built with modern web technologies and powered by AI, it aims to enhance independence and safety.

## ‚ú® Core Features

*   **Auditory Welcome**: A spoken welcome message greets the user when the application loads, providing immediate auditory feedback and orientation.
*   **Real-time Object & Scene Description**:
    *   **Activation**: Double-tap the screen to activate the camera.
    *   **Functionality**: Periodically captures frames from the device camera. These frames are sent to a sophisticated AI flow (`describeSceneFlow`) that analyzes the image and generates a textual description of the scene.
    *   **Feedback**: The generated description is then spoken aloud to the user.
    *   **Deactivation**: Double-tap the screen again to stop the camera and description.
*   **Voice-Activated Personal Assistant**:
    *   **Activation**: Tap and hold the screen to start the personal assistant.
    *   **Functionality**: Transcribes the user's speech in real-time. The transcribed text, along with the user's current location (if permission is granted), is sent to an AI flow (`personalAssistantFlow`).
    *   **Feedback**: The assistant's response, generated by the AI, is spoken aloud.
    *   **Continuous Interaction**: The assistant continues to listen for further commands or queries until the user taps and holds the screen again to deactivate it.
*   **Content Relevance Tool**: An underlying AI-powered tool (`contentRelevanceFlow`) used to evaluate and decide which web content is most relevant to present when providing the user with important information, ensuring focused and useful results.
*   **Intuitive Gesture Navigation**:
    *   **Double Tap**: Toggles the Real-time Object/Scene Description feature.
    *   **Tap and Hold**: Toggles the Voice-Activated Personal Assistant.
    *   *(Future gestures could be added for more functionalities)*
*   **Responsive Design**: The interface is designed to be responsive and accessible on various devices, including mobile.

## üöÄ Tech Stack

*   **Frontend**:
    *   [Next.js](https://nextjs.org/): React framework for server-side rendering, static site generation, and a great developer experience.
    *   [React](https://reactjs.org/): JavaScript library for building user interfaces.
    *   [TypeScript](https://www.typescriptlang.org/): Superset of JavaScript that adds static typing.
*   **UI Components**:
    *   [ShadCN UI](https://ui.shadcn.com/): Beautifully designed, accessible, and customizable components built with Radix UI and Tailwind CSS.
    *   [Tailwind CSS](https://tailwindcss.com/): A utility-first CSS framework for rapid UI development.
    *   [Lucide React](https://lucide.dev/): Simply beautiful open-source icons.
*   **Artificial Intelligence (AI)**:
    *   [Genkit (by Google)](https://firebase.google.com/docs/genkit): An open-source framework for building AI-powered features, used here for the core AI flows (scene description, personal assistant, content relevance).
    *   The backend AI flows are assumed to be deployed (e.g., on Vercel, Google Cloud Run) and are accessed via API calls.
*   **Mobile Deployment (Optional but intended)**:
    *   [Capacitor](https://capacitorjs.com/): Cross-platform native runtime for building web apps that run natively on iOS, Android, and the Web.
*   **Styling**:
    *   Follows specific color guidelines (calming blue, neutral gray, gentle green) for high contrast and accessibility.

## üõ†Ô∏è Getting Started

Follow these instructions to get a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

*   [Node.js](https://nodejs.org/) (version 18.x or later recommended)
*   [npm](https://www.npmjs.com/) (usually comes with Node.js)
*   Git

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/YOUR_USERNAME/assistivevisions.git
    cd assistivevisions
    ```
    *(Replace `YOUR_USERNAME/assistivevisions.git` with the actual URL of your repository if it's different)*

2.  **Install dependencies**:
    ```bash
    npm install
    ```

3.  **Set up Environment Variables**:
    Create a `.env.local` file in the root of your project and add any necessary environment variables. For example, if your Genkit flows are deployed and require an API key or have a specific base URL:
    ```env
    # .env.local
    NEXT_PUBLIC_GENKIT_API_BASE_URL=https_your_deployed_genkit_api_endpoint/api/assistive
    # Add any other environment variables required by your Genkit flows or other services
    ```
    The application is currently hardcoded to use `https://assistive-api.vercel.app/api/assistive`. If you deploy your own Genkit backend, update the API endpoint in the relevant frontend files (e.g., `hooks/useAssistiveLogic.tsx` or similar) or preferably manage it via an environment variable.

### Running the Development Server

To run the Next.js development server:
```bash
npm run dev
```
Open [http://localhost:3000](http://localhost:3000) with your browser to see the application.

### Building for Production

To build the Next.js application for production:
```bash
npm run build
```
This will create an optimized build in the `.next` folder. To run the production build locally:
```bash
npm run start
```

## üì± Capacitor (for Mobile App Development)

If you intend to build a native mobile app (Android/iOS) using Capacitor:

### 1. Initial Capacitor Setup (if not already done)

*   Install Capacitor CLI globally (optional, but can be helpful):
    ```bash
    npm install -g @capacitor/cli
    ```
*   Initialize Capacitor in your project (if you haven't already):
    ```bash
    npx cap init AssistiveVisions com.example.assistivevisions
    ```
    (Replace `com.example.assistivevisions` with your desired app ID)
*   Install necessary Capacitor platforms:
    ```bash
    npm install @capacitor/android @capacitor/ios
    npx cap add android
    npx cap add ios
    ```

### 2. Configure `next.config.js` for Static Export

Ensure your `next.config.js` is set up for static export (`output: 'export'`) as Capacitor requires this.

```js
// next.config.js
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: 'export', // Important for Capacitor
  images: {
    unoptimized: true // Important for Capacitor static export
  },
  // ... other configurations
};

module.exports = nextConfig;
```

### 3. Build Your Next.js App

```bash
npm run build
```
This command will generate the static assets in the `out` directory (due to `output: 'export'`).

### 4. Sync with Capacitor

```bash
npx cap sync
```
This command copies your web build (from the `out` directory) into the native Capacitor projects.

### 5. Open Native IDE

*   **For Android**:
    ```bash
    npx cap open android
    ```
    This will open your project in Android Studio. From there, you can build and run your app on an emulator or a physical device. Make sure you have the Android SDK and necessary build tools installed.

*   **For iOS**:
    ```bash
    npx cap open ios
    ```
    This will open your project in Xcode. From there, you can build and run your app on an emulator or a physical device. You'll need a macOS machine and Xcode installed.

### Important Considerations for Mobile:

*   **Permissions**: You'll need to configure permissions for Camera and Microphone in the native project files (`AndroidManifest.xml` for Android, `Info.plist` for iOS). The provided `AndroidManifest.xml` template should include these.
*   **Native APIs**: For features like Text-to-Speech (TTS) and Speech-to-Text (STT), you'll ideally use Capacitor plugins or native implementations for better performance and reliability than relying solely on web APIs. The current `AssistiveHomeLogicAdapter.tsx` or similar hook uses browser-based APIs which might have limitations on mobile. Consider plugins like:
    *   `@capacitor/text-to-speech`
    *   `@capacitor-community/speech-recognition`
*   **API Calls**: Ensure your deployed Genkit API backend is accessible from the mobile device (CORS might need to be configured on your backend if it's different from the web deployment).

## ü§ù Contributing

Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag "enhancement".
Don't forget to give the project a star! Thanks again!

1.  **Fork the Project**
2.  **Create your Feature Branch** (`git checkout -b feature/AmazingFeature`)
3.  **Commit your Changes** (`git commit -m 'Add some AmazingFeature'`)
4.  **Push to the Branch** (`git push origin feature/AmazingFeature`)
5.  **Open a Pull Request**

## üìú License

This project is unlicensed and in the public domain. You are free to use, modify, and distribute the code as you see fit. However, if you choose to use a license, the [MIT License](https://opensource.org/licenses/MIT) or [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0) are good choices for open-source projects.

**To formally add a license:**
1.  Choose a license (e.g., MIT).
2.  Create a `LICENSE` file in the root of your project.
3.  Copy the text of your chosen license into this file.
4.  Update this section in the README to reflect your chosen license.

## üìß Contact

Asad/IIT BHU Intern Project - beingasad47@gmail.com (Optional)

Project Link: [https://github.com/asad8210/Assistive-Seeing-Eyes](https://github.com/asad8210/Assistive-Seeing-Eyes)


## üôè Acknowledgements 

*   Prof. Sanjay Kumar Singh Sir (IIT BHU) and Ankita Chand Ma'am (Phd. Scholar IIT BHU) Who helped or inspired me for the projects as a Internship.
*   Links to resources that were particularly helpful for Blind People.
*   Feature is to reduce life hardness of whose Visually Impaired People.

    
